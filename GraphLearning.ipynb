{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in this script, we implement our algorithm for matrix learning using different regularizers.\n",
    "# In order to run these algorithms, Mosek has to be installed and the user must posses a valid Mosek licence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using JuMP \n",
    "using Mosek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Spectral learning<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# @param:weights : weights of the observed matrices. Sum of weights must be 1. and non-negative. the size is equal to the amount\n",
    "#of observed graphs.\n",
    "#@param: delta : size of the ball of the Wasserstein metric. delta > 0.\n",
    "#@param: A : cluster form of the data. A entries must be 0 or 1. \n",
    "#@param: B : vector of matrices containing the observations of the graph B. If X,Y are matrices, an array of matrices\n",
    "#is of the form Array[X,Y].\n",
    "#@param: n : the dimension of the matrices.\n",
    "#@param:N : number of observations.\n",
    "#The set over which the is problem is optimized is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#display(\"text/latex\",\"\"\" El conjunto sobre el que optimizamos es:\n",
    "#\\$ O = \\\\{\\\\lambda,S_1,..,S_N,Z^{1},..,Z^{N},C^{1},...,C^{N},W,W_1,W_2 \\\\}\\\\subseteq \\\\mathbb{R}\\\\times \\\\mathbb{R}^N\n",
    "#\\\\times Sim^2(\\\\mathbb{R}^n)^N \\\\times Sim^2(\\\\mathbb{R}^n)^N \\\\times Sim^2(\\\\mathbb{R}^)^3\n",
    "#\\$\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learnError (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computes the error of estimating the cluster structure of the observations by the matrix A.\n",
    "function learnSpectralError(weights,delta,A,B,n)\n",
    "    N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "     @variable(m,S[1:N])\n",
    "    @variable(m, W[1:n,1:n], Symmetric)\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    C = @variable(m, [1:n, 1:n], Symmetric)\n",
    "    @variable(m,W1[1:n,1:n])\n",
    "    @variable(m,W2[1:n,1:n])\n",
    "    #Constraints for Z\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #Constraints for C \n",
    "    @constraint(m,C.>=-(2*A-one*one'-W))\n",
    "    @constraint(m,C.>=0)\n",
    "    [@constraint(m, one'*C*one<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    #Constraints for W\n",
    "    @SDconstraint(m,[W1 W;W W2]>=0)\n",
    "    @constraint(m,trace(W1)+trace(W2)<=2*lambda)\n",
    "\n",
    "    #Objective function, calls the function prod\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+produ.(Z,n)))\n",
    "    status = solve(m)\n",
    "    return([getvalue(C),getvalue(lambda),getvalue(S),getobjectivevalue(m)])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learnGraph (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deprecated. use instead  learnEspectralGraph\n",
    "#Finds the matrix A which minimices the expected error of cluster structure. This\n",
    "#time, A is a simmtric entrywise-positive matrix.\n",
    "#The parameters are the same as in learnError function\n",
    "function learnGraph(weights,delta,B,n)\n",
    "    N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "     @variable(m,S[1:N])\n",
    "    @variable(m, W[1:n,1:n], Symmetric)\n",
    "    @variable(m,A[1:n,1:n], Symmetric)\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    C = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    @variable(m,W1[1:n,1:n])\n",
    "    @variable(m,W2[1:n,1:n])\n",
    "    #Constraints for Z\n",
    "    [@constraint(m, -Z[p][k,l] <= A[k,l]-B[p][k,l]) for p in 1:N for k in 1:n for l in 1:n]\n",
    "    [@constraint(m,  A[k,l]-B[p][k,l]<=Z[p][k,l]) for p in 1:N for k in 1:n for l in 1:n]\n",
    "    #Constraints for C \n",
    "    [@constraint(m,C[p][k,l]>=0)for p in 1:N for k in 1:n for l in 1:n]\n",
    "    [@constraint(m, produ(C[p],n)<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    [@constraint(m,-C[p][k,l]<=2*A[k,l]-1-W[k,l])for p in 1:N for k in 1:n for l in 1:n]\n",
    "    #Constraints for W\n",
    "    @SDconstraint(m,[W1 W;W W2]>=0)\n",
    "    @constraint(m,trace(W1)+trace(W2)<=2*lambda)\n",
    "    #constrains for A\n",
    "    [@constraint(m,A[k,l]>=0) for k in 1:n for l in 1:n]\n",
    "    [@constraint(m,A[k,l]<=1) for k in 1:n for l in 1:n]\n",
    "    #Objective function, calls the function prod\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+produ.(Z,n)))\n",
    "    status = solve(m)\n",
    "    return([roundZeroMatrix(getvalue(A)),getvalue(S),getvalue(lambda),roundZeroMatrix(getvalue(W)),getobjectivevalue(m)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finds the matrix A which minimices the expected error of cluster structure. This\n",
    "#time, A is a simmtric entrywise-positive matrix.\n",
    "#The parameters are the same as in learnError function\n",
    "function learnEspectralGraph(weights,delta,B,n)\n",
    "    N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "     @variable(m,S[1:N])\n",
    "    @variable(m, W[1:n,1:n], Symmetric)\n",
    "    @variable(m,A[1:n,1:n], Symmetric)\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    C = @variable(m, [1:n, 1:n], Symmetric)\n",
    "    @variable(m,W1[1:n,1:n])\n",
    "    @variable(m,W2[1:n,1:n])\n",
    "    #Constraints for Z\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #Constraints for C \n",
    "    @constraint(m,C.>=-(2*A-one*one'-W))\n",
    "    @constraint(m,C.>=0)\n",
    "    [@constraint(m, one'*C*one<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    #Constraints for W\n",
    "    @SDconstraint(m,[W1 W;W W2]>=0)\n",
    "    @constraint(m,trace(W1)+trace(W2)<=2*lambda)\n",
    "    #constrains for A\n",
    "    @constraint(m,A.>=0)\n",
    "    @constraint(m,A.<=1)\n",
    "    #Objective function, calls the function prod\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+produ.(Z,n)))\n",
    "    status = solve(m)\n",
    "    return([getvalue(A),getvalue(S),getvalue(lambda),getvalue(W),getobjectivevalue(m)])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>L2 learning<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mUndefVarError: @variable not defined\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mUndefVarError: @variable not defined\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "#Finds the matrix A which minimices the expected error of cluster structure. This\n",
    "#time, A is a simmtric entrywise-positive matrix.\n",
    "#The parameters are the same as in learnError function\n",
    "function learnl2Graph(weights,delta,B,n)\n",
    " N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "    @variable(m,S[1:N])\n",
    "    @variable(m,A[1:n,1:n], Symmetric)\n",
    "    #W debe ser simetrica? Creo que no.\n",
    "    @variable(m, W[1:n,1:n], Symmetric)\n",
    "    #Q es una variable auxiliar, juega el papel de Lambda en la demostracion\n",
    "    @variable(m,Q[1:n,1:n])\n",
    "    # u es una variable auxiliar para calcular la norma infinito de W.\n",
    "   # @variable(m,u>=0)\n",
    "    #Z es un conjunto de matrices que permite calcular la norma 1 de A-B[i].\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    " \n",
    "    #Constrains...\n",
    "    #constrains for lambda already in the definition of the varible.\n",
    "    #Constrains for A.\n",
    "    @constraint(m,A.>=0)\n",
    "    @constraint(m,A.<=1)\n",
    "    #Constrains for Z.\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #Constrains for W. \n",
    "    @constraint(m,vec(W)'vec(W)<=lambda)\n",
    "     #Constrains for Q.\n",
    "    @constraint(m,Q .>=-(2*A-one*one'-W))\n",
    "    @constraint(m,Q .>=0)\n",
    "    [@constraint(m, one'*Q*one<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    #Objetivo\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+produ.(Z,n)))\n",
    "    #TT = STDOUT # save original STDOUT stream\n",
    "    #redirect_stdout()\n",
    "    solve(m)\n",
    "    #redirect_stdout(TT) # restore STDOUT\n",
    "    return([getvalue(A),getvalue(S),getvalue(lambda),getvalue(W),getobjectivevalue(m)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function learnl2Error(weights,delta,A,B,n)\n",
    " N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "    @variable(m,S[1:N])\n",
    "    #W debe ser simetrica? Creo que no.\n",
    "    @variable(m, W[1:n,1:n], Symmetric)\n",
    "    #Q es una variable auxiliar, juega el papel de Lambda en la demostracion\n",
    "    @variable(m,Q[1:n,1:n])\n",
    "    # u es una variable auxiliar para calcular la norma infinito de W.\n",
    "   # @variable(m,u>=0)\n",
    "    #Z es un conjunto de matrices que permite calcular la norma 1 de A-B[i].\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    " \n",
    "    #Constrains...\n",
    "    #constrains for lambda already in the definition of the varible.\n",
    "\n",
    "    #Constrains for Z.\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #Constrains for W. \n",
    "    @constraint(m,vec(W)'vec(W)<=lambda)\n",
    "     #Constrains for Q.\n",
    "    @constraint(m,Q .>=-(2*A-one*one'-W))\n",
    "    @constraint(m,Q .>=0)\n",
    "    [@constraint(m, one'*Q*one<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    #Objetivo\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+produ.(Z,n)))\n",
    "    #TT = STDOUT # save original STDOUT stream\n",
    "    #redirect_stdout()\n",
    "    solve(m)\n",
    "    #redirect_stdout(TT) # restore STDOUT\n",
    "    return([getvalue(Q),getvalue(lambda),getvalue(S),getobjectivevalue(m)])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Linear learning<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @param:weights : weights of the observed matrices. Sum of weights must be 1. and non-negative. the size is equal to the amount\n",
    "#of observed graphs.\n",
    "#@param: delta : size of the ball of the Wasserstein metric. delta > 0.\n",
    "#@param: A : cluster form of the data. A entries must be 0 or 1. \n",
    "#@param: B : vector of matrices containing the observations of the graph B. If X,Y are matrices, an array of matrices\n",
    "#is of the form Array[X,Y].\n",
    "#@param: n : the dimension of the matrices.\n",
    "#@param:N : number of observations.\n",
    "#The set over which the is problem is optimized is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function learnLinearGraph(weights,delta,B,n)\n",
    "    N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "    @variable(m,S[1:N])\n",
    "    @variable(m,A[1:n,1:n], Symmetric)\n",
    "    #W debe ser simetrica? Creo que no.\n",
    "    @variable(m, W[1:n,1:n],Symmetric)\n",
    "    #Q es una variable auxiliar, juega el papel de Lambda en la demostracion\n",
    "    @variable(m,Q[1:n,1:n],Symmetric)\n",
    "    # u es una variable auxiliar para calcular la norma infinito de W.\n",
    "    @variable(m,u>=0)\n",
    "    #Z es un conjunto de matrices que permite calcular la norma 1 de A-B[i].\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    " \n",
    "    #Constrains...\n",
    "    #constrains for lambda already in the definition of the varible.\n",
    "    #Constrains for A.\n",
    "    @constraint(m,A.>=0)\n",
    "    @constraint(m,A.<=1)\n",
    "    #Constrains for Z.\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #Constrains for W. \n",
    "    @constraint(m,u<=lambda)\n",
    "    @constraint(m,u .>=W)\n",
    "    @constraint(m,u .>=-W)\n",
    "     #Constrains for Q.\n",
    "    @constraint(m,Q .>=-(2*A-one*one'-W))\n",
    "    @constraint(m,Q .>=0)\n",
    "    [@constraint(m, one'*Q*one<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    #Objetivo\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+produ.(Z,n)))\n",
    "    #TT = STDOUT # save original STDOUT stream\n",
    "    #redirect_stdout()\n",
    "    solve(m)\n",
    "    #redirect_stdout(TT) # restore STDOUT\n",
    "    return([getvalue(A),getvalue(S),getvalue(lambda),getvalue(W),getobjectivevalue(m)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function learnLinearError(weights,delta,A,B,n)\n",
    "    N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "    @variable(m,S[1:N])\n",
    "    #W debe ser simetrica? Creo que no.\n",
    "    @variable(m, W[1:n,1:n],Symmetric)\n",
    "    #Q es una variable auxiliar, juega el papel de Lambda en la demostracion\n",
    "    @variable(m,Q[1:n,1:n])\n",
    "    # u es una variable auxiliar para calcular la norma infinito de W.\n",
    "    @variable(m,u>=0)\n",
    "    #Z es un conjunto de matrices que permite calcular la norma 1 de A-B[i].\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    #Constrains...\n",
    "    #constrains for lambda already in the definition of the varible.\n",
    "    #Constrains for Z.\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #Constrains for W. \n",
    "    @constraint(m,u<=lambda)\n",
    "    @constraint(m,u .>=W)\n",
    "    @constraint(m,u .>=-W)\n",
    "    #Constrains for Q.\n",
    "    @constraint(m,Q .>=-(2*A-one*one'-W))\n",
    "    @constraint(m,Q .>=0)\n",
    "    [@constraint(m, ones(n)'*Q*ones(n)<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    #Objetivo\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+produ.(Z,n)))\n",
    "    #TT = STDOUT # save original STDOUT stream\n",
    "    #redirect_stdout()\n",
    "    solve(m)\n",
    "    #redirect_stdout(TT) # restore STDOUT\n",
    "    return([getvalue(Q),getvalue(lambda),getvalue(S),getobjectivevalue(m)])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Regularization learning<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Funcion para calcular el lado derecho de la desigualdad, i.e min A \n",
    "# 1/m (sum ||A-B_i||_1) +delta(||2A-11^t||_nuc)\n",
    "#asumimos que los pesos son todos iguales a 1/N\n",
    "function learnRegularized2A(weights,delta,B,n)\n",
    "    N = length(B)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,A[1:n,1:n], Symmetric)\n",
    "    @variable(m,W1[1:n,1:n])\n",
    "    @variable(m,W2[1:n,1:n])\n",
    "    @variable(m,W[1:n,1:n],Symmetric)\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    #Constraints for Z\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #constrains for A\n",
    "    @constraint(m,A.>=0)\n",
    "    @constraint(m,A.<=1)\n",
    "    @constraint(m,W.>=2*A-ones(n)*ones(n)')\n",
    "    @constraint(m,W.<=2*A-ones(n)*ones(n)')\n",
    "    @SDconstraint(m,[W1 W;W W2]>=0)\n",
    "    @objective(m,Min, dot(weights,produ.(Z,n))+delta*(0.5*(trace(W1)+trace(W2))))\n",
    "     status = solve(m)\n",
    "    return([getvalue(A),getvalue(W),getvalue(dot(weights,produ.(Z,n))),getvalue(Z),getobjectivevalue(m)])\n",
    "end\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#funcion para calcular #1/m (sum ||A-B_i||_1) +delta(||2A-11^t||_nuc)\n",
    "function learnRegularized2AError(A,weights,delta,B,n)\n",
    "    N = length(B)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,W1[1:n,1:n])\n",
    "    @variable(m,W2[1:n,1:n])\n",
    "    @variable(m,W[1:n,1:n],Symmetric)\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    #Constraints for Z\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    @constraint(m,W.>=2*A-ones(n)*ones(n)')\n",
    "    @constraint(m,W.<=2*A-ones(n)*ones(n)')\n",
    "    @SDconstraint(m,[W1 W;W W2]>=0)\n",
    "    @objective(m,Min, dot(weights,produ.(Z,n))+delta*(0.5*(trace(W1)+trace(W2))))\n",
    "     status = solve(m)\n",
    "    return([getobjectivevalue(m),getvalue(W),getvalue(Z)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Funcion para calcular el lado derecho de la desigualdad, i.e min A \n",
    "# 1/m (sum ||A-B_i||_1) +delta(||A||_nuc)\n",
    "#asumimos que los pesos son todos iguales a 1/N\n",
    "function learnRegularizedA(weights,delta,B,n)\n",
    "    N = length(B)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,A[1:n,1:n], Symmetric)\n",
    "    @variable(m,W1[1:n,1:n])\n",
    "    @variable(m,W2[1:n,1:n])\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    #Constraints for Z\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #constrains for A\n",
    "    @constraint(m,A.>=0)\n",
    "    @constraint(m,A.<=1)\n",
    "    @SDconstraint(m,[W1 A;A W2]>=0)\n",
    "    @objective(m,Min, dot(weights,produ.(Z,n))+delta*(0.5*(trace(W1)+trace(W2))))\n",
    "     status = solve(m)\n",
    "    return([getvalue(A),getvalue(dot(weights,produ.(Z,n))),getvalue(Z),getobjectivevalue(m)])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Funcion para calcular el lado derecho de la desigualdad, i.e \n",
    "#funcion para calcular #1/m (sum ||A-B_i||_1) +delta(||A||_nuc)\n",
    "#asumimos que los pesos son todos iguales a 1/N\n",
    "function learnRegularizedA(weights,delta,B,n)\n",
    "    N = length(B)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,A[1:n,1:n], Symmetric)\n",
    "    @variable(m,W1[1:n,1:n])\n",
    "    @variable(m,W2[1:n,1:n])\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    #Constraints for Z\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #constrains for A\n",
    "    @constraint(m,A.>=0)\n",
    "    @constraint(m,A.<=1)\n",
    "    @SDconstraint(m,[W1 A;A W2]>=0)\n",
    "    @objective(m,Min, dot(weights,produ.(Z,n))+delta*(0.5*(trace(W1)+trace(W2))))\n",
    "     status = solve(m)\n",
    "    return([getvalue(A),getvalue(dot(weights,produ.(Z,n))),getvalue(Z),getobjectivevalue(m)])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
